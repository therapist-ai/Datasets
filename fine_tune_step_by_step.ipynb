{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "56483e72",
      "metadata": {
        "id": "56483e72",
        "outputId": "cf3d7003-a64d-41a0-c36a-6be99bbc2a49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.25.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (4.0.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4.0,>=2.0\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Installing collected packages: multidict, frozenlist, charset-normalizer, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 charset-normalizer-3.1.0 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.2 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "import json\n",
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f27d4ed",
      "metadata": {
        "id": "3f27d4ed"
      },
      "source": [
        "# How to fine-tune a GPT-3 model for specific prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b56a7b0",
      "metadata": {
        "id": "6b56a7b0"
      },
      "source": [
        "I'm constantly looking for ways to automate the work with support requests. An idea has been to fine-tune a GPT-3 model to answer common support-related questions.\n",
        "\n",
        "**Here's how you can fine-tune a GPT-3 model with Python with your own data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f831ab",
      "metadata": {
        "id": "a2f831ab"
      },
      "source": [
        "In this walkthrough, we'll fine-tune a GPT-3 model to answer common support-related questions.\n",
        "\n",
        "Detailed step-by-step intructions for this repo in this blog post: https://norahsakal.com/blog/fine-tune-gpt3-model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b102018b",
      "metadata": {
        "id": "b102018b"
      },
      "source": [
        ">### Disclaimer\n",
        ">This guide walks you through fine-tuning a GPT-3 model in Python, shown in a Jupyter notebook.\n",
        ">If you're looking for the steps of fine-tuning right in a terminal, [OpenAI has a great guide for fine-tuning in your terminal](https://beta.openai.com/docs/guides/fine-tuning \"fine-tuning in terminal\")."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "698f669e",
      "metadata": {
        "id": "698f669e"
      },
      "source": [
        "# Define OpenAI API keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dcaff120",
      "metadata": {
        "id": "dcaff120"
      },
      "outputs": [],
      "source": [
        "api_key =\"YOUR_OPENAI_API_KEY\"\n",
        "openai.api_key = api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10f08dd7",
      "metadata": {
        "id": "10f08dd7"
      },
      "source": [
        "# Create training data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "569f0c1f",
      "metadata": {
        "id": "569f0c1f"
      },
      "source": [
        "Make sure to end each `prompt` with a suffix. According to the [OpenAI API reference](https://beta.openai.com/docs/guides/fine-tuning \"fine-tuning reference\"), you can use ` ->`.\n",
        "\n",
        "Also, make sure to end each `completion` with a suffix as well; I'm using `.\\n`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ca5904b9",
      "metadata": {
        "id": "ca5904b9"
      },
      "outputs": [],
      "source": [
        "data_file = [{\n",
        "    \"prompt\": \"Prompt ->\",\n",
        "    \"completion\": \" Ideal answer.\\n\"\n",
        "},{\n",
        "    \"prompt\":\"Prompt ->\",\n",
        "    \"completion\": \" Ideal answer.\\n\"\n",
        "}]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e10cb45e",
      "metadata": {
        "id": "e10cb45e"
      },
      "source": [
        "# Save dict as JSONL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c6ed465",
      "metadata": {
        "id": "7c6ed465"
      },
      "source": [
        "Training data need to be a JSONL document.\n",
        "JSONL file is a newline-delimited JSON file.\n",
        "More info about JSONL: https://jsonlines.org/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4e4bc0cf",
      "metadata": {
        "id": "4e4bc0cf"
      },
      "outputs": [],
      "source": [
        "file_name = \"training_data.jsonl\"\n",
        "\n",
        "with open(file_name, 'w') as outfile:\n",
        "    for entry in data_file:\n",
        "        json.dump(entry, outfile)\n",
        "        outfile.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cbf169e",
      "metadata": {
        "id": "6cbf169e"
      },
      "source": [
        "# Check JSONL file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7a5fe452",
      "metadata": {
        "id": "7a5fe452",
        "outputId": "e3f0925b-8e82-4fad-db5f-e38c92e3e9ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing...\n",
            "\n",
            "- Your file contains 2 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
            "- There are 1 duplicated prompt-completion sets. These are rows: [1]\n",
            "\n",
            "ERROR in common_suffix validator: All prompts are identical: `Prompt ->`\n",
            "Consider leaving the prompts blank if you want to do open-ended generation, otherwise ensure prompts are different\n",
            "\n",
            "Aborting..."
          ]
        }
      ],
      "source": [
        "!openai tools fine_tunes.prepare_data -f training_data.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b923b02a",
      "metadata": {
        "id": "b923b02a"
      },
      "source": [
        "# Upload file to your OpenAI account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa6f8267",
      "metadata": {
        "id": "fa6f8267"
      },
      "outputs": [],
      "source": [
        "upload_response = openai.File.create(\n",
        "  file=open(file_name, \"rb\"),\n",
        "  purpose='fine-tune'\n",
        ")\n",
        "upload_response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20fb4254",
      "metadata": {
        "id": "20fb4254"
      },
      "source": [
        "# Save file name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93469f37",
      "metadata": {
        "id": "93469f37"
      },
      "outputs": [],
      "source": [
        "file_id = upload_response.id\n",
        "file_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e1007a6",
      "metadata": {
        "id": "3e1007a6"
      },
      "source": [
        "# Fine-tune a model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57e2da9b",
      "metadata": {
        "id": "57e2da9b"
      },
      "source": [
        "The default model is **Curie**. \n",
        "\n",
        "If you'd like to use **DaVinci** instead, then add it as a base model to fine-tune:\n",
        "\n",
        "```openai.FineTune.create(training_file=file_id, model=\"davinci\")```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16bb42a2",
      "metadata": {
        "id": "16bb42a2"
      },
      "outputs": [],
      "source": [
        "fine_tune_response = openai.FineTune.create(training_file=file_id)\n",
        "fine_tune_response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2059b2b3",
      "metadata": {
        "id": "2059b2b3"
      },
      "source": [
        "# Check fine-tune progress"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc7fda90",
      "metadata": {
        "id": "cc7fda90"
      },
      "source": [
        "Check the progress with `openai.FineTune.list_events(id=fine_tune_response.id)` and get a list of all the fine-tuning events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cf062ab",
      "metadata": {
        "id": "4cf062ab"
      },
      "outputs": [],
      "source": [
        "fine_tune_events = openai.FineTune.list_events(id=fine_tune_response.id)\n",
        "fine_tune_events"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "362f5bf3",
      "metadata": {
        "id": "362f5bf3"
      },
      "source": [
        "Check the progress with `openai.FineTune.retrieve(id=fine_tune_response.id)` and get an object with the fine-tuning job data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f01831be",
      "metadata": {
        "id": "f01831be"
      },
      "outputs": [],
      "source": [
        "retrieve_response = openai.FineTune.retrieve(id=fine_tune_response.id)\n",
        "retrieve_response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e846ed",
      "metadata": {
        "id": "07e846ed"
      },
      "source": [
        "# Save fine-tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ca86136",
      "metadata": {
        "id": "6ca86136"
      },
      "source": [
        "### Troubleshooting fine_tuned_model as null\n",
        "During the fine-tuning process, the **fine_tuned_model** key may not be immediately available in the fine_tune_response object returned by `openai.FineTune.create()`.\n",
        "\n",
        "To check the status of your fine-tuning process, you can call the `openai.FineTune.retrieve()` function and pass in the **fine_tune_response.id**. This function will return a JSON object with information about the training status, such as the current epoch, the current batch, the training loss, and the validation loss.\n",
        "\n",
        "After the fine-tuning process is complete, you can check the status of all your fine-tuned models by calling `openai.FineTune.list()`. This will list all of your fine-tunes and their current status.\n",
        "\n",
        "Once the fine-tuning process is complete, you can retrieve the fine_tuned_model key by calling the `openai.FineTune.retrieve()` function again and passing in the fine_tune_response.id. This will return a JSON object with the key fine_tuned_model and the ID of the fine-tuned model that you can use for further completions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a071cb90",
      "metadata": {
        "id": "a071cb90"
      },
      "source": [
        "### Option 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e7e760b",
      "metadata": {
        "id": "6e7e760b"
      },
      "source": [
        "If `fine_tune_response.fine_tuned_model != None` then the key **fine_tuned_model** is availble from the fine_tune_response object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0616dd72",
      "metadata": {
        "id": "0616dd72"
      },
      "outputs": [],
      "source": [
        "if fine_tune_response.fine_tuned_model != None:\n",
        "    fine_tuned_model = fine_tune_response.fine_tuned_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34613188",
      "metadata": {
        "id": "34613188"
      },
      "source": [
        "### Option 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3635655e",
      "metadata": {
        "id": "3635655e"
      },
      "source": [
        "If `fine_tune_response.fine_tuned_model == None:` you can get the **fine_tuned_model** by listing all fine-tune events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77d2f317",
      "metadata": {
        "id": "77d2f317"
      },
      "outputs": [],
      "source": [
        "if fine_tune_response.fine_tuned_model == None:\n",
        "    fine_tune_list = openai.FineTune.list()\n",
        "    fine_tuned_model = fine_tune_list['data'][0].fine_tuned_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "619d0d25",
      "metadata": {
        "id": "619d0d25"
      },
      "source": [
        "### Option 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f13537bb",
      "metadata": {
        "id": "f13537bb"
      },
      "source": [
        "If `fine_tune_response.fine_tuned_model == None:` you can get the **fine_tuned_model** key by retrieving the fine-tune job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e6203b",
      "metadata": {
        "id": "35e6203b"
      },
      "outputs": [],
      "source": [
        "if fine_tune_response.fine_tuned_model == None:\n",
        "    fine_tuned_model = openai.FineTune.retrieve(id=fine_tune_response.id).fine_tuned_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8601df11",
      "metadata": {
        "id": "8601df11"
      },
      "source": [
        "# Test the new model on a new prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c654268c",
      "metadata": {
        "id": "c654268c"
      },
      "source": [
        "Remember to end the prompt with the same suffix as we used in the training data; ` ->`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37cfb2da",
      "metadata": {
        "id": "37cfb2da"
      },
      "outputs": [],
      "source": [
        "new_prompt = \"NEW PROMPT ->\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bee69cb8",
      "metadata": {
        "id": "bee69cb8"
      },
      "outputs": [],
      "source": [
        "answer = openai.Completion.create(\n",
        "  model=fine_tuned_model,\n",
        "  prompt=new_prompt,\n",
        "  max_tokens=10, # Change amount of tokens for longer completion\n",
        "  temperature=0\n",
        ")\n",
        "answer['choices'][0]['text']"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}